{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c39dcb7",
   "metadata": {},
   "source": [
    "# Train Tokenizer\n",
    "There are two popular encoding choices: character encoding and sub-word encoding. Sub-word encoding models are almost nearly identical to the character encoding models. The primary difference lies in the fact that a sub-word encoding model accepts a sub-word tokenized text corpus and emits sub-word tokens in its decoding step. \n",
    "Preparation of the tokenizer is made simple by the [process_asr_text_tokenizer.py script](https://github.com/NVIDIA/NeMo/blob/main/scripts/tokenizers/process_asr_text_tokenizer.py) in NeMo. We leverage this script to build the text corpus from the manifest directly, then create a tokenizer using that corpus.\n",
    "\n",
    "## Subword Tokenization\n",
    "\n",
    "If you are familiar with Natural Language Processing, then you might have heard of the term \"<i>subword</i>\" frequently. So what is a subword in the first place? Simply put, it is either a single character or a group of characters. When combined according to a tokenization-detokenization algorithm, it generates a set of characters, words, or entire sentences. \n",
    "\n",
    "Many subword tokenization-detokenization algorithms exist, which can be built using large corpora of text data to tokenize and detokenize the data to and from subwords effectively. Some of the most commonly used subword tokenization methods are [Byte Pair Encoding](https://arxiv.org/abs/1508.07909), [Word Piece Encoding](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf) and [Sentence Piece Encoding](https://www.aclweb.org/anthology/D18-2012/).\n",
    "\n",
    "## The necessity of subword tokenization for ASR\n",
    "\n",
    "It has been found via extensive research in the domain of Neural Machine Translation and Language Modelling that subword tokenization not only reduces the length of the tokenized representation (thereby making sentences shorter and more manageable for models to learn), but also boosts the accuracy of prediction of correct tokens.\n",
    "\n",
    "The [Connectionist Temporal Classification](https://www.cs.toronto.edu/~graves/icml_2006.pdf) loss function is commonly used to train acoustic models, but this loss function has a few limitations:\n",
    "\n",
    " - **Generated tokens are conditionally independent of each other**. In other words - the probability of character \"l\" being predicted after \"hel##\" is conditionally independent of the previous token - so any other token can also be predicted unless the model has future information!\n",
    " - **The length of the generated (target) sequence must be shorter than that of the source sequence.** \n",
    "\n",
    "It turns out - subword tokenization helps alleviate both of these issues!\n",
    "\n",
    " - Sophisticated subword tokenization algorithms build their vocabularies based on large text corpora. To accurately tokenize such large volumes of text with minimal vocabulary size, the subwords that are learned inherently model the interdependency between tokens of that language to some degree. \n",
    " \n",
    "Looking at the previous example, the token `hel##` is a single token that represents the relationship `h` => `e` => `l`. When the model predicts the singe token `hel##`, it implicitly predicts this relationship - even though the subsequent token can be either `l` (for `hell`) or `##lo` (for `hello`) and is predicted independently of the previous token!\n",
    "\n",
    " - By reducing the target sentence length by subword tokenization (target sentence here being the characters/subwords transcribed from the audio signal), we entirely sidestep the sequence length limitation of CTC loss!\n",
    "\n",
    "This means we can perform a larger number of pooling steps in our acoustic models, thereby improving execution speed while simultaneously reducing memory requirements.\n",
    "\n",
    "First, download the tokenizer creation script from the nemo repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ee4841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BRANCH = 'main'\n",
    "if not os.path.exists(\"scripts/process_asr_text_tokenizer.py\"):\n",
    "  !mkdir scripts\n",
    "  !wget -P scripts/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/scripts/tokenizers/process_asr_text_tokenizer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3969851e",
   "metadata": {},
   "source": [
    "The script above takes a few important arguments -\n",
    "\n",
    " - either `--manifest` or `--data_file`: If your text data lies inside of an ASR manifest file, then use the `--manifest` path. If instead the text data is inside a file with separate lines corresponding to different text lines, then use `--data_file`. In either case, you can add commas to concatenate different manifests or different data files.\n",
    "\n",
    " - `--data_root`: The output directory (whose subdirectories will be created if not present) where the tokenizers will be placed.\n",
    "\n",
    " - `--vocab_size`: The size of the tokenizer vocabulary. Larger vocabularies can accommodate almost entire words, but the decoder size of any model will grow proportionally.\n",
    "\n",
    " - `--tokenizer`: Can be either `spe` or  `wpe` . `spe` refers to the Google `sentencepiece` library tokenizer. `wpe` refers to the HuggingFace BERT Word Piece tokenizer. Please refer to the papers above for the relevant technique in order to select an appropriate tokenizer.\n",
    "\n",
    " - `--no_lower_case`: When this flag is passed, it will force the tokenizer to create separate tokens for upper and lower case characters. By default, the script will turn all the text to lower case before tokenization (and if upper case characters are passed during training/inference, the tokenizer will emit a token equivalent to Out-Of-Vocabulary). Used primarily for the English language. \n",
    "\n",
    " - `--spe_type`: The `sentencepiece` library has a few implementations of the tokenization technique, and `spe_type` refers to these implementations. Currently supported types are `unigram`, `bpe`, `char`, `word`. Defaults to `bpe`.\n",
    "\n",
    " - `--spe_character_coverage`: The `sentencepiece` library considers how much of the original vocabulary it should cover in its \"base set\" of tokens (akin to the lower and upper case characters of the English language). For almost all languages with small base token sets `(<1000 tokens)`, this should be kept at its default of 1.0. For languages with larger vocabularies (say Japanese, Mandarin, Korean etc), the suggested value is 0.9995.\n",
    "\n",
    " - `--spe_sample_size`: If the dataset is too large, consider using a sampled dataset indicated by a positive integer. By default, any negative value (default = -1) will use the entire dataset.\n",
    "\n",
    " - `--spe_train_extremely_large_corpus`: When training a sentencepiece tokenizer on very large amounts of text, sometimes the tokenizer will run out of memory or wont be able to process so much data on RAM. At some point you might receive the following error - \"Input corpus too large, try with train_extremely_large_corpus=true\". If your machine has large amounts of RAM, it might still be possible to build the tokenizer using the above flag. Will silently fail if it runs out of RAM.\n",
    "\n",
    " - `--log`: Whether the script should display log messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2af1478d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Corpus already exists at path : ./data/processed/tokenizer/text_corpus/document.txt\n",
      "WARNING:root:Model file already exists, overriding old model file !\n",
      "[NeMo I 2022-05-31 04:45:16 sentencepiece_tokenizer:307] Processing ./data/processed/tokenizer/text_corpus/document.txt and store at ./data/processed/tokenizer/tokenizer_spe_bpe_v1024\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=./data/processed/tokenizer/text_corpus/document.txt --model_prefix=./data/processed/tokenizer/tokenizer_spe_bpe_v1024/tokenizer --vocab_size=1024 --shuffle_input_sentence=true --hard_vocab_limit=false --model_type=bpe --character_coverage=1.0 --bos_id=-1 --eos_id=-1 --normalization_rule_name=nmt_nfkc_cf\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./data/processed/tokenizer/text_corpus/document.txt\n",
      "  input_format: \n",
      "  model_prefix: ./data/processed/tokenizer/tokenizer_spe_bpe_v1024/tokenizer\n",
      "  model_type: BPE\n",
      "  vocab_size: 1024\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 0\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: -1\n",
      "  eos_id: -1\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc_cf\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: ./data/processed/tokenizer/text_corpus/document.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 320761 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=31598519\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=190\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 320761 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 320761\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 282404\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1003983 min_freq=7\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=182773 size=20 all=3113 active=2055 piece=‚ñÅun\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=114326 size=40 all=4184 active=3126 piece=or\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=76769 size=60 all=5901 active=4843 piece=‚ñÅp\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=57486 size=80 all=7385 active=6327 piece=‚ñÅt\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=40333 size=100 all=8941 active=7883 piece=ro\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=39332 min_freq=3008\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=30669 size=120 all=10185 active=2095 piece=‚ñÅsein\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=27036 size=140 all=11647 active=3557 piece=sp\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=22870 size=160 all=13507 active=5417 piece=nen\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=19236 size=180 all=14965 active=6875 piece=‚ñÅwur\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16646 size=200 all=16750 active=8660 piece=‚ñÅje\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=16521 min_freq=2674\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14713 size=220 all=18723 active=2940 piece=√∂n\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12767 size=240 all=20284 active=4501 piece=‚ñÅzur\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11829 size=260 all=21656 active=5873 piece=he\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10571 size=280 all=23957 active=8174 piece=‚ñÅherr\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9791 size=300 all=25275 active=9492 piece=‚ñÅbes\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=9776 min_freq=1795\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9026 size=320 all=27091 active=3049 piece=olit\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8354 size=340 all=28611 active=4569 piece=‚ñÅfra\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7699 size=360 all=30348 active=6306 piece=‚ñÅmuss\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7277 size=380 all=31427 active=7385 piece=gr\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6804 size=400 all=32887 active=8844 piece=ke\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=6788 min_freq=1256\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6466 size=420 all=34437 active=3048 piece=ichtig\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6222 size=440 all=35418 active=4029 piece=innen\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5849 size=460 all=36385 active=4996 piece=utz\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5494 size=480 all=37690 active=6301 piece=‚ñÅeigen\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5223 size=500 all=38747 active=7358 piece=iz\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=5196 min_freq=988\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5011 size=520 all=40219 active=3268 piece=halt\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4778 size=540 all=41665 active=4714 piece=oll\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4496 size=560 all=43340 active=6389 piece=hin\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4276 size=580 all=44516 active=7565 piece=√§nder\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4145 size=600 all=45817 active=8866 piece=‚ñÅhaus\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=4133 min_freq=753\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3960 size=620 all=47131 active=3557 piece=hem\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3782 size=640 all=48451 active=4877 piece=‚ñÅbez\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3689 size=660 all=48996 active=5422 piece=‚ñÅdaf√ºr\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3585 size=680 all=49482 active=5908 piece=‚ñÅanderen\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3449 size=700 all=50558 active=6984 piece=‚ñÅbl\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=3443 min_freq=652\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3335 size=720 all=51886 active=3822 piece=tern\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3219 size=740 all=53217 active=5153 piece=unkt\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3113 size=760 all=54560 active=6496 piece=ahn\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2993 size=780 all=55506 active=7442 piece=bau\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2897 size=800 all=56636 active=8572 piece=spro\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=2883 min_freq=549\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2833 size=820 all=57612 active=3718 piece=‚ñÅra\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: ./data/processed/tokenizer/tokenizer_spe_bpe_v1024/tokenizer.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: ./data/processed/tokenizer/tokenizer_spe_bpe_v1024/tokenizer.vocab\n",
      "Serialized tokenizer at location : ./data/processed/tokenizer/tokenizer_spe_bpe_v1024\n",
      "INFO:root:Done!\n"
     ]
    }
   ],
   "source": [
    "!python ./scripts/process_asr_text_tokenizer.py --manifest=./data/processed/train_manifest_merged.json \\\n",
    "         --data_root=./data/processed/tokenizer \\\n",
    "         --vocab_size=1024 \\\n",
    "         --tokenizer=\"spe\" \\\n",
    "         --log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757b946b",
   "metadata": {},
   "source": [
    "That's it! Our tokenizer is now built and stored inside the `data_root` directory that we provided to the script.\n",
    "\n",
    "We can inspect the tokenizer vocabulary itself. To keep it manageable, we will print just the first 10 tokens of the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "411c9240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##en\n",
      "##er\n",
      "##ch\n",
      "d\n",
      "##ei\n",
      "##ie\n",
      "s\n",
      "##un\n",
      "a\n",
      "w\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 ./data/processed/tokenizer/tokenizer_spe_bpe_v1024/vocab.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
