{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/riva_asr_asr-python-advanced-tao-ngram-pretrain/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to pretrain a Riva ASR Language Modeling (n-gram) with TAO Toolkit\n",
    "This notebook is a walkthrough of  pretraining the Riva ASR language model (n-gram) with [NVIDIA Train Adapt Optimize (TAO)](https://developer.nvidia.com/tao) Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAO Toolkit\n",
    "Train Adapt Optimize (TAO) Toolkit is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data. Developers, researchers, and software partners building intelligent vision AI applications and services can bring their own data to fine-tune pre-trained models instead of going through the hassle of training from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Train Adapt Optimize (TAO) Toolkit](https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning extracts learned features from an existing neural network into a new one. Transfer learning is often used when creating a large training dataset is not feasible. The goal of this toolkit is to reduce that 80 hour workload to an 8 hour workload, which can enable data scientists to have considerably more train-test iterations in the same time frame.\n",
    "\n",
    "Let's see this in action with a use case for Automatic Speech Recognition Language Modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='isc-task-description'></a>\n",
    "## Language Modeling\n",
    "\n",
    "### Task Description\n",
    "\n",
    "Language modeling returns a probability distribution over a sequence of words. Besides assigning a probability to a sequence of words, the language models also assign a probability for the likelihood of a given word (or a sequence of words) that follows a sequence of words. <br>\n",
    "\n",
    "> The sentence:  **all of a sudden I notice three guys standing on the sidewalk**\n",
    "> would be scored higher than \n",
    "> the sentence: **on guys all I of notice sidewalk three a sudden standing the** by the language model. <br>\n",
    "\n",
    "A language model trained on large corpus can significantly improve the accuracy of an ASR system as suggested in recent research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are primarily two types of language models:\n",
    "\n",
    "- **N-gram language models**: These models use frequency of n-grams to learn the probability distribution over words. Two benefits of N-gram language models are simplicity and scalability – with a larger `n`, a model can store more context with a well-understood space–time tradeoff, enabling small experiments to scale up efficiently.\n",
    "- **Neural language models**: These models use different kinds of neural networks to model the probability distribution over words, and have surpassed the N-gram language models in the ability to model language, but are generally slower to evaluate.\n",
    "\n",
    "In this tutorial, we will show how to train, evaluate, and optionally fine-tune an [**N-gram language model**](https://web.stanford.edu/~jurafsky/slp3/3.pdf) leveraging TAO Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Let's Dig in: Riva Language Modeling using TAO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing and setting up TAO\n",
    "Install TAO Toolkit inside the Python virtual environment.\n",
    "\n",
    "It's a simple `pip` install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nvidia-pyindex\n",
    "!pip install nvidia-tao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the Docker image versions and the tasks that TAO can perform, use the `tao info` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao info --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='isc-prepare-data'></a>\n",
    "### Preparing the dataset\n",
    "#### Librispeech LM Normalized dataset\n",
    "For this tutorial, we use the **normalized version of the LibriSpeech LM dataset** to **train** our N-gram language model. The normalized version of the LibriSpeech LM dataset is available [here](https://www.openslr.org/11/).\n",
    "\n",
    "#### LibriSpeech dev-clean dataset\n",
    "For this tutorial, we also use the **clean version of the LibriSpeech development set** to **evaluate** our N-gram language model. The clean version of the LibriSpeech development set is available [here](https://www.openslr.org/12/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the dataset\n",
    "#### LibriSpeech LM Normalized dataset\n",
    "The training data is publicly available [here](https://www.openslr.org/resources/11/librispeech-lm-corpus.tgz) and can be downloaded directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "\n",
    "# Create a local directory to save artifacts in this tutorial\n",
    "LM_ARTIFACTS = os.path.join(os.getcwd(), \"lm-pretraining-artifacts\")\n",
    "!mkdir -p $LM_ARTIFACTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to a folder where you want your data and results to be saved.\n",
    "DATA_DOWNLOAD_DIR = os.path.join(LM_ARTIFACTS, \"data\")\n",
    "\n",
    "!mkdir -p $DATA_DOWNLOAD_DIR\n",
    "\n",
    "assert os.path.exists(DATA_DOWNLOAD_DIR), \"Provided DATA_DOWNLOAD_DIR does not exist.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Ensure that wget and unzip utilities are available. If not, install them.\n",
    "if os.path.exists(os.path.join(DATA_DOWNLOAD_DIR, \"librispeech-lm-norm.txt\")):\n",
    "    print(\"Dataset exists, skipping download\")\n",
    "else:\n",
    "    print(\"Downloading and Extracting the Data\")\n",
    "    !wget 'https://www.openslr.org/resources/11/librispeech-lm-norm.txt.gz' -P $DATA_DOWNLOAD_DIR\n",
    "    !gzip -dk $DATA_DOWNLOAD_DIR/librispeech-lm-norm.txt.gz;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LibriSpeech dev-clean dataset\n",
    "The evaluation data is publicly available [here](https://www.openslr.org/resources/12/dev-clean.tar.gz) and can be downloaded directly. We provide a Python script below to download and preprocess the dataset for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scripts to download and preprocess LibriSpeech dev-clean\n",
    "\"\"\"\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy\n",
    "\n",
    "LOG_STR = \" To regenerate this file, please, remove it.\"\n",
    "\n",
    "def find_transcript_files(dir):\n",
    "    files = []\n",
    "    for dirpath, _, filenames in os.walk(dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".trans.txt\"):\n",
    "                files.append(os.path.join(dirpath, filename))\n",
    "    return files\n",
    "\n",
    "def transcript_to_list(file):\n",
    "    audio_path = os.path.dirname(file)\n",
    "    ret = []\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            file_id, trans = line.strip().split(\" \", 1)\n",
    "            audio_file = os.path.abspath(os.path.join(audio_path, file_id + \".flac\"))\n",
    "            duration = 0  # We are not using the audio\n",
    "            ret.append([file_id, audio_file, str(duration), trans.lower()])\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    name = \"dev-clean\"\n",
    "    data_path = os.path.join(DATA_DOWNLOAD_DIR, \"eval_data\")\n",
    "    text_path = os.path.join(DATA_DOWNLOAD_DIR, \"text\")\n",
    "    lists_path = os.path.join(DATA_DOWNLOAD_DIR, \"lists\")\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    os.makedirs(text_path, exist_ok=True)\n",
    "    os.makedirs(lists_path, exist_ok=True)\n",
    "    data_http = \"http://www.openslr.org/resources/12/\"\n",
    "\n",
    "    # Download the audio data\n",
    "    print(\"Downloading the evaluation data.\", flush=True)\n",
    "    if not os.path.exists(os.path.join(data_path, \"LibriSpeech\", name)):\n",
    "        print(\"Downloading and unpacking {}...\".format(name))\n",
    "        cmd = \"\"\"wget -c {http}{name}.tar.gz -P {path};\n",
    "                 yes n 2>/dev/null | gunzip {path}/{name}.tar.gz;\n",
    "                 tar -C {path} -xf {path}/{name}.tar\"\"\"\n",
    "        os.system(cmd.format(path=data_path, http=data_http, name=name))\n",
    "    else:\n",
    "        log_str = \"{} part of data exists, skip its downloading and unpacking\"\n",
    "        print(log_str.format(name) + LOG_STR, flush=True)\n",
    "\n",
    "    # Prepare the audio data\n",
    "    print(\"Converting data into necessary format.\", flush=True)\n",
    "    word_dict = {}\n",
    "    word_dict[name] = set()\n",
    "    src = os.path.join(data_path, \"LibriSpeech\", name)\n",
    "    assert os.path.exists(src), \"Unable to find the directory - '{src}'\".format(\n",
    "        src=src\n",
    "    )\n",
    "\n",
    "    dst_list = os.path.join(lists_path, name + \".lst\")\n",
    "    if os.path.exists(dst_list):\n",
    "        print(\n",
    "            \"Path {} exists, skip its generation.\".format(dst_list) + LOG_STR,\n",
    "            flush=True,\n",
    "        )\n",
    "        \n",
    "\n",
    "    print(\"Analyzing {src}...\".format(src=src), flush=True)\n",
    "    transcript_files = find_transcript_files(src)\n",
    "    transcript_files.sort()\n",
    "\n",
    "    print(\"Writing to {dst}...\".format(dst=dst_list), flush=True)\n",
    "    with Pool(processes=8) as p:\n",
    "        samples = list(p.imap(transcript_to_list, transcript_files))\n",
    "\n",
    "    with open(dst_list, \"w\") as fout:\n",
    "        for sp in samples:\n",
    "            for s in sp:\n",
    "                word_dict[name].update(s[-1].split(\" \"))\n",
    "                s[0] = name + \"-\" + s[0]\n",
    "                fout.write(\" \".join(s) + \"\\n\")\n",
    "\n",
    "    current_path = os.path.join(text_path, name + \".txt\")\n",
    "    if not os.path.exists(current_path):\n",
    "        with open(os.path.join(lists_path, name + \".lst\"), \"r\") as flist, open(\n",
    "            os.path.join(text_path, name + \".txt\"), \"w\"\n",
    "        ) as fout:\n",
    "            for line in flist:\n",
    "                fout.write(\" \".join(line.strip().split(\" \")[3:]) + \"\\n\")\n",
    "    else:\n",
    "        print(\n",
    "            \"Path {} exists, skip its generation.\".format(current_path) + LOG_STR,\n",
    "            flush=True,\n",
    "        )\n",
    "\n",
    "print(\"Done!\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of reducing the time this tutorial takes, we reduce the number of lines of the training dataset. Feel free to modify the number of used lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a random 10,000 lines for training\n",
    "!shuf -n 10000 $DATA_DOWNLOAD_DIR/librispeech-lm-norm.txt  > $DATA_DOWNLOAD_DIR/reduced_training.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 5 $DATA_DOWNLOAD_DIR/reduced_training.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TAO Toolkit workflow\n",
    "The rest of the tutorial demonstrates what a sample TAO Toolkit workflow looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting TAO Toolkit Mounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataset has been downloaded, an important step in using TAO Toolkit is to setup the directory mounts. The TAO Toolkit launcher uses Docker containers under the hood, and **for our data and results directory to be visible to Docker, they need to be mapped**. The launcher can be configured using the config file `~/.tao_mounts.json`. Apart from the mounts, you can also configure additional options like the environment variables and the amount of shared memory available to the TAO Toolkit launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The following code creates a sample `~/.tao_mounts.json`  file. Here, we can map directories in which we save the data, specs, results, and cache. You should configure it for your specific use case.  These directories are correctly visible to the Docker container. **Ensure that the source directories exist on your machine.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -ltr {LM_ARTIFACTS}/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define these paths on your local host machine\n",
    "%env HOST_DATA_DIR={LM_ARTIFACTS}/data\n",
    "%env HOST_SPECS_DIR={LM_ARTIFACTS}/specs\n",
    "%env HOST_RESULTS_DIR={LM_ARTIFACTS}/results\n",
    "%env HOST_CACHE_DIR={LM_ARTIFACTS}/cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create these directories if they don't already exist\n",
    "!mkdir -p {LM_ARTIFACTS}/specs\n",
    "!mkdir -p {LM_ARTIFACTS}/results\n",
    "!mkdir -p {LM_ARTIFACTS}/cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the local directories to the TAO docker.\n",
    "import json\n",
    "import os\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "tlt_configs = {\n",
    "   \"Mounts\":[\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_DATA_DIR\"],\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_SPECS_DIR\"],\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_RESULTS_DIR\"],\n",
    "           \"destination\": \"/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.path.expanduser(\"~/.cache\"),\n",
    "           \"destination\": \"/root/.cache\"\n",
    "       }\n",
    "   ],\n",
    "   \"DockerOptions\": {\n",
    "        \"shm_size\": \"16G\",\n",
    "        \"ulimits\": {\n",
    "            \"memlock\": -1,\n",
    "            \"stack\": 67108864\n",
    "         }\n",
    "   }\n",
    "}\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(tlt_configs, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users with basic knowledge of deep learning can get started building their own custom models using a simple specification file. It's essentially just one command each to run data preprocessing, training, fine-tuning, evaluation, inference, and export. All configurations happen through `.yaml` spec files. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Configuration/Specification Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The essence of all commands in TAO Toolkit lies within `.yaml` spec files. There are sample spec files already available for you to use directly or as a reference to create your own.  Through these spec files, you can tune many knobs like the model, dataset, hyperparameters, etc. Each command (like train, fine-tune, evaluate, etc.) should have a dedicated spec file with configurations pertinent to it. <br>\n",
    "\n",
    "Here is an example of the training spec file:\n",
    "\n",
    "---\n",
    "```\n",
    "model:\n",
    "  intermediate: True\n",
    "  order: 2\n",
    "  pruning:\n",
    "    - 0\n",
    "training_ds:\n",
    "  is_tarred: false\n",
    "  is_file: true\n",
    "  data_file: ???\n",
    "\n",
    "vocab_file: \"\"\n",
    "encryption_key: \"tlt_encode\"\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Set Relevant Paths\n",
    "Please set these paths according to your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TAO Toolkit Docker. \n",
    "\n",
    "# The data is saved here\n",
    "DATA_DIR='/data'\n",
    "\n",
    "# The configuration files are stored here\n",
    "SPECS_DIR='/specs/n_gram'\n",
    "\n",
    "# The results are saved at this path\n",
    "RESULTS_DIR='/results/n_gram'\n",
    "\n",
    "# Set your encryption key, and use the same key for all commands\n",
    "KEY='tlt_encode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### Downloading Specs\n",
    "Let's download the spec files. You may choose to modify/rewrite these specs, or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command. <br>\n",
    "\n",
    "The `-o` argument indicates the folder where the default specification files will be downloaded. The `-r` argument instructs the script on where to save the logs. **Ensure the `-o` argument points to an empty folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(LM_ARTIFACTS, \"specs/n_gram\")):\n",
    "    print(\"Downloading Specs\")\n",
    "    !tao n_gram download_specs \\\n",
    "        -r $RESULTS_DIR \\\n",
    "        -o $SPECS_DIR\n",
    "else:\n",
    "    print(\"n_gram .yaml specs already exist. If you want to re-download, please remove the contents of this directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Data Convert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparation for training/fine-tuning, we need to preprocess the dataset. The `tao n_gram dataset_convert` command can be used in conjunction with the appropriate configuration in the spec file. Here is the sample `dataset_convert.yaml` spec file we use:\n",
    "```\n",
    "# Dataset. Available options: [assistant]\n",
    "dataset_name: assistant\n",
    "\n",
    "# Extension of the files containing in dataset\n",
    "extension: ???\n",
    "\n",
    "# Path to the folder containing the dataset source files.\n",
    "source_data_dir: ???\n",
    "\n",
    "# Path to the output folder.\n",
    "target_data_file: ???\n",
    "\n",
    "```\n",
    " Take a look at the `.yaml` spec files we provide.\n",
    "As we show below, you can override the `source_data_dir` and `target_data_dir` options with the appropriate paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess training data (LibriSpeech LM Normalized)\n",
    "!tao n_gram dataset_convert \\\n",
    "            -e $SPECS_DIR/dataset_convert.yaml \\\n",
    "            -r $RESULTS_DIR/dataset_convert \\\n",
    "            extension=*.txt \\\n",
    "            source_data_dir=$DATA_DIR/reduced_training.txt \\\n",
    "            target_data_file=$DATA_DIR/preprocessed.txt\n",
    "\n",
    "# Preprocess evaluation data (LibriSpeech dev-clean)\n",
    "!tao n_gram dataset_convert \\\n",
    "            -e $SPECS_DIR/dataset_convert.yaml \\\n",
    "            -r $RESULTS_DIR/dataset_convert \\\n",
    "            extension=*.txt \\\n",
    "            source_data_dir=$DATA_DIR/text/dev-clean.txt \\\n",
    "            target_data_file=$DATA_DIR/preprocessed_dev_clean.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command preprocess the training and evaluation dataset using basic text preprocessings including converting lowercase, normalization, removing punctuation, and write the results into files named `preprocessed.txt` and `preprocessed_dev_clean.txt` for training and evaluation correspondingly. In both `preprocessed.txt` and `preprocessed_dev_clean.txt` files, each preprocessed sentence corresponds to a new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek into the preprocessed dataset\n",
    "!head -n 5 $DATA_DOWNLOAD_DIR/preprocessed.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='isc-training'></a>\n",
    "### Training / Fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model using TAO Toolkit is as simple as configuring your spec file and running the train command. The following code uses the `train.yaml` spec file available to you as reference. The spec file configurations can easily be overridden using the `tao-launcher` CLI. For example, below we override `model.order`, `model.pruning` and `training_ds.data_file` configurations to suit our needs. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_YAML = os.path.join(LM_ARTIFACTS, \"specs\", \"n_gram\", \"train.yaml\")\n",
    "!cat $TRAIN_YAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some parameters you can modify/add specific to n_gram training - <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parameter                 | Data Type   | Default | Description |\n",
    "| -----------               | ----------- |-------- |-----------  |\n",
    "| training_ds.data_file     | string      | -       |Path to dataset file. |\n",
    "| model.order               | int         | -       | Order of N-Gram model (maximum number of grams) |\n",
    "| vocab_file                | string      | -       | Optional path to vocab file to limit vocabulary learned by model. |\n",
    "| model.intermediate        | boolean     | true    | Choose from [true,false]. If True, creates intermediate file - required for finetune and interpolate |\n",
    "| model.pruning             | list[int]   | [0]     | Prune grams with counts less than or equal to threhold provided for each gram. Non-decreasing. Starts with 0 |\n",
    "| export_to                 | string      | -        | The path to the trained .tlt model |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For training an N-gram language model in TAO Toolkit, we use the `tao n_gram train` command with the following general TAO arguments:\n",
    "- `-e`: Path to the spec file\n",
    "- `-k`: User specified encryption key to use while saving/loading the model\n",
    "- `-r`: Path to a folder where the outputs should be written. Ensure this is mapped in the `tlt_mounts.json` file.\n",
    "- Any overrides to the spec file. For example, `model.order`.\n",
    "<br>\n",
    "\n",
    "\n",
    "For more information about these arguments, refer to the [TAO Toolkit Getting Started Guide](https://docs.nvidia.com/tao/tao-toolkit/text/overview.html). <br>\n",
    "`Note:` All file paths correspond to the destination mounted directory that is visible in the TAO Toolkit docker container used in backend.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we traing a 3-gram model\n",
    "!tao n_gram train \\\n",
    "            -e $SPECS_DIR/train.yaml \\\n",
    "            -r $RESULTS_DIR/train \\\n",
    "            training_ds.data_file=$DATA_DIR/preprocessed.txt \\\n",
    "            model.order=3 \\\n",
    "            model.pruning=[0,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train command produces results saved at `$RESULTS_DIR/n_gram/train/checkpoints`, including three files called `train_n_gram.arpa`, `train_n_gram.vocab` and `train_n_gram.kenlm_intermediate` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the generated artifacts at Local path\n",
    "!ls -ltr $LM_ARTIFACTS/results/n_gram/train/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='evaluation'></a>\n",
    "### Evaluation\n",
    "The evaluation spec `.yaml` is as simple as:\n",
    "\n",
    "```\n",
    "# Name of the `.arpa` or `.binary` file where the trained model will be restored from.\n",
    "restore_from: ???\n",
    "\n",
    "test_ds:\n",
    "  data_file: ???\n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao n_gram evaluate \\\n",
    "     -e $SPECS_DIR/evaluate.yaml \\\n",
    "     -r $RESULTS_DIR/evaluate \\\n",
    "     restore_from=$RESULTS_DIR/train/checkpoints/train_n_gram.arpa \\\n",
    "     test_ds.data_file=$DATA_DIR/preprocessed_dev_clean.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the evaluation gives us the **perplexity** of the N-gram language model on the evaluation (LibriSpeech dev-clean) dataset.\n",
    "\n",
    "#### A note on perplexity\n",
    "\n",
    "Language models are typically evaluated not using raw probabilities, but with the [perplexity](https://en.wikipedia.org/wiki/Perplexity) metric. The perplexity (PP) of a language model on a test set is the inverse probability of the test set, normalized by the number of words. Such normalization is important because different datasets can have different number of sentences and words in each each sentences. Without normalization, a larger test set will have lower probabilities. Perplexity is independent of the size of the test set.\n",
    "\n",
    "For a given test set $W = w_1 w_2 .. w_N$ , <br> \n",
    "    Perplexity is defined as <br>\n",
    "$PP(w) = P(w_1 w_2 .. w_N)^{-1/N}$\n",
    "\n",
    "Because of the inverse probability, the higher the probability of a sentence, the lower the perplexity. Therefore, with perplexity, lower is better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='isc-inference'></a>\n",
    "### Inference\n",
    "Now, we execute inference using a trained `.arpa` or `.binary` model uses the `tao n_gram infer` command.  <br>\n",
    "The `infer.yaml` is also very simple, and we can directly give inputs for the model to run inference.\n",
    "```\n",
    "# \"Simulate\" user input:\n",
    "input_batch:\n",
    "  - 'set alarm for seven thirty am'\n",
    "  - 'lower volume by fifty percent'\n",
    "  - 'what is my schedule for tomorrow'\n",
    "\n",
    "restore_from: ???\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao n_gram infer \\\n",
    "            -e $SPECS_DIR/infer.yaml \\\n",
    "            -r $RESULTS_DIR/infer \\\n",
    "            restore_from=$RESULTS_DIR/train/checkpoints/train_n_gram.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command returns the **log likelihood**, **perplexity**, and all n-grams for each of the input sequences that users provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='isc-export-riva'></a>\n",
    "### Export to Riva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With TAO Toolkit, you can also export your model in a format that can deployed using [NVIDIA Riva](https://developer.nvidia.com/riva). The export command will convert the trained language model from `.arpa` to `.binary` with the option of quantizing the model binary. We will set `export_format` in the spec file to `RIVA` to create a `.riva` file which will contain the language model binary and its corresponding vocabulary.\n",
    "\n",
    "`NOTE:` More information about the different arguments can be found in the [TAO documentation](https://docs.nvidia.com/tao/tao-toolkit/text/lm/n_gram.html?highlight=binary_q#model-export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao n_gram export \\\n",
    "            -e $SPECS_DIR/export.yaml \\\n",
    "            -r $RESULTS_DIR/export \\\n",
    "            export_format=RIVA \\\n",
    "            export_to=exported-model.riva \\\n",
    "            restore_from=$RESULTS_DIR/train/checkpoints/train_n_gram.arpa \\\n",
    "            binary_type=trie \\\n",
    "            binary_q_bits=8 \\\n",
    "            binary_b_bits=7 \\\n",
    "            binary_a_bits=256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is exported as `exported-model.binary` which is in a format suited for deployment in Riva. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploying the exported n-gram language model in the speech recognition pipeline is similar to the steps as specified in the 1_deploy-speech-recognition-pipeline.ipynb. Deploying it again is not part of this tutorial. <br>\n",
    "\n",
    "For reference, you can point the `--decoding_language_model_binary` arg in `riva-build` to your freshly exported language model.\n",
    "\n",
    "After `riva-build` and `riva-deploy`, you can follow the rest of the tutorial `1_deploy-speech-recognition-pipeline.ipynb` to deploy your newly generated language model along with the downloaded acoustic model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
