{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/riva_asr_asr-python-advanced-finetune-am-conformer-ctc-for-noisy-audio-withtao/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to improve accuracy on specific speech patterns by fine-tuning the Acoustic Model (Conformer-CTC) in the Riva ASR pipeline \n",
    "\n",
    "This tutorial walks you through some of the advanced customization features of the Riva ASR pipeline by fine-tuning the Acoustic Model (Conformer-CTC). These customization features improve accuracy on specific speech patterns, like background noise and different acoustic environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVIDIA Riva Overview\n",
    "\n",
    "NVIDIA Riva is a GPU-accelerated SDK for building Speech AI applications that are customized for your use case and deliver real-time performance. <br/>\n",
    "Riva offers a rich set of speech and natural language understanding services such as:\n",
    "\n",
    "- Automated speech recognition (ASR)\n",
    "- Text-to-Speech synthesis (TTS)\n",
    "- A collection of natural language processing (NLP) services, such as named entity recognition (NER), punctuation, and intent classification.\n",
    "\n",
    "In this tutorial, we will customize the Riva ASR pipeline by fine-tuning the Acoustic Model (Conformer-CTC) with NVIDIA's TAO Toolkit to improve accuracy on audio with background noise.  \n",
    "To understand the basics of Riva ASR APIs, refer to [Getting started with Riva ASR in Python](https://github.com/nvidia-riva/tutorials/blob/stable/asr-python-basics.ipynb). <br>\n",
    "\n",
    "For more information about Riva, refer to the Riva [product page](https://developer.nvidia.com/riva) and [documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/overview.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Riva Acoustic Model (Conformer-CTC) with NVIDIA TAO\n",
    "\n",
    "The following flow diagram shows the Riva speech recognition pipeline along with all the possible customizations. \n",
    "\n",
    "Raw temporal audio signals first pass through a feature extraction block which segments the data into blocks (for example, of 80 ms each), then converts the blocks from temporal domain to frequency domain (MFCC). This data is then fed into an acoustic model which outputs probabilities over text tokens at each time step. A decoder converts this matrix of probabilities into a sequence of text tokens which is then `detokenized` into an actual sentence (or character sequence). An advanced decoder can also do beam search and score multiple possible hypotheses (i.e. sentences) in conjunction with a language model. The decoder output comes without punctuation and capitalization, which is the job of the Punctuation and Capitalization model. Finally, Inverse Text Normalization (ITN) rules are applied to transform the text in verbal format into a desired written format.\n",
    "\n",
    "<img src=\"./imgs/riva-asr-customizations-amfinetuning.PNG\" style=\"float: center;\">\n",
    "\n",
    "For this tutorial, we need to fine-tune the pre-trained Riva acoustic model. \n",
    "\n",
    "There are multiple options available for the acoustic model with Riva - Conformer-CTC, Citrinet, Jasper, and Quartznet. In this tutorial we are going to use the Conformer-CTC model and demonstrate how it can be fine-tuned.  \n",
    "Fine-tuning a Conformer-CTC model is not yet supported. Support for this is planned in a future release.    \n",
    "For more information about these acoustic models and when to use them, refer to the Riva documentation [here](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/reference/models/asr.html).\n",
    "\n",
    "You can use NVIDIA TAO Toolkit to fine-tune the Conformer-CTC acoustic model in the Riva ASR pipeline.\n",
    "\n",
    "#### NVIDIA TAO Toolkit Overview\n",
    "\n",
    "NVIDIA Train Adapt Optimize (TAO) Toolkit is a python-based AI toolkit for transfer learning that takes purpose-built pre-trained AI models and customizes them on your own data. TAO enables developers with limited AI expertise to create highly accurate AI models for production deployments.  \n",
    "TAO follows zero coding paradigm. There is no need to write any code to train models with TAO. Training can be done by just running a few commands with the TAO command-line interface.  \n",
    "\n",
    "Riva supports fine-tuning with TAO. The fine-tuned TAO model can easily be deployed for real-time inference on the Riva Speech Skills server.\n",
    "\n",
    "For more information about the NVIDIA TAO framework, refer to the documentation [here](https://docs.nvidia.com/tao/tao-toolkit/text/overview.html).\n",
    "\n",
    "### Fine-tune the Conformer-CTC model with NVIDIA TAO:\n",
    "\n",
    "The process of fine-tuning a Riva Conformer-CTC acoustic model with NVIDIA TAO can be split into three steps:\n",
    "1. Data preprocessing.\n",
    "2. Fine-tuning the Conformer-CTC model with TAO.\n",
    "3. Deploying the fine-tuned Conformer-CTC TAO model on the Riva Speech Skills server.\n",
    "Let's walk through each of these steps in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Data preprocessing\n",
    "\n",
    "For fine-tuning we need audio data with background noise. If you already have such data, then you can use it directly.  \n",
    "In this tutorial, we will take the AN4 dataset and augment it with noise data from the Room Impulse Response and Noise Database from the [openslr database](https://www.openslr.org/28/).\n",
    "NVIDIA TAO Toolkit does not currently support audio data augmentation. This support will be added in a future release.\n",
    "In this tutorial, we will be using NVIDIA NeMo for the data preprocessing step.\n",
    "\n",
    "#### NVIDIA NeMo Overview\n",
    "\n",
    "NVIDIA NeMo is a toolkit for building new state-of-the-art conversational AI models. NeMo has separate collections for Automatic Speech Recognition (ASR), Natural Language Processing (NLP), and Text-to-Speech (TTS) models. Each collection consists of prebuilt modules that include everything needed to train on your data. Every module can easily be customized, extended, and composed to create new conversational AI model architectures.\n",
    "For more information about NeMo, refer to the [NeMo product page](https://developer.nvidia.com/nvidia-nemo) and [documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/starthere/intro.html). The open-source NeMo repository can be found [here](https://github.com/NVIDIA/NeMo).\n",
    "\n",
    "NVIDIA NeMo and NVIDIA TAO are both training toolkits. TAO abstracts the training details from the user, whereas NeMo exposes them. TAO follows the zero-coding paradigm, therefore, TAO is better suited for users who want to quickly fine-tune models on their custom dataset. NeMo is the preferred option for researches.  \n",
    "TAO is the preferred training toolkit for Riva because of it's ease-of-use.\n",
    "\n",
    "In this tutorial, we will be using NeMo only for data preprocessing. We will use the TAO Toolkit for the actual training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements and setup for data preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Requirements:\n",
    "\n",
    "We will be using [NVIDIA NeMo](https://github.com/NVIDIA/NeMo) for this data preprocessing step - Easiest way to install and run NeMo is through NVIDIA's PyTorch docker container. If you are not already running this notebook in the NVIDIA PyTorch docker container, please follow instructions [here](./README.md#running-the-nvidia-riva-tutorial-how-to-improve-accuracy-on-specific-speech-patterns-by-fine-tuning-the-acoustic-model-citrinet-in-the-riva-asr-pipeline) to re-run this tutorial from the PyTorch docker container\n",
    "\n",
    "We will be using the NVIDIA NeMo Docker container, so access to NGC is a must. As of this writing, the latest (22.08) version of the container does not work for the data preprocessing, but the previous version (22.05) does. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Download and process the AN4 dataset\n",
    "AN4 is a small dataset recorded and distributed by Carnegie Mellon University (CMU). It consists of recordings of people spelling out addresses, names, etc. Information about this dataset can be found on the official CMU site.\n",
    "\n",
    "Let's download the AN4 dataset tar file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the working directory for this part of the tutorial. \n",
    "working_dir = 'am_finetuning/'\n",
    "!mkdir -p $working_dir\n",
    "\n",
    "# Import the necessary dependencies.\n",
    "import wget\n",
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "import tarfile\n",
    "\n",
    "# The AN4 directory will be created in `data_dir`. It is currently set to the `working_dir`.\n",
    "data_dir = os.path.abspath(working_dir)\n",
    "\n",
    "# Download the AN4 dataset if it doesn't already exist in `data_dir`. \n",
    "# This will take a few moments...\n",
    "# We also set `an4_path` which points to the downloaded an4 dataset\n",
    "if not os.path.exists(data_dir + '/an4_sphere.tar.gz'):\n",
    "    an4_url = 'https://dldata-public.s3.us-east-2.amazonaws.com/an4_sphere.tar.gz'\n",
    "    an4_path = wget.download(an4_url, data_dir)\n",
    "    print(f\"AN4 dataset downloaded at: {an4_path}\")\n",
    "else:\n",
    "    print(\"AN4 dataset tarfile already exists. Proceed to the next step.\")\n",
    "    an4_path = data_dir + '/an4_sphere.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's untar the tar file to give us the dataset audio files in `.sph` format. Then, we'll convert the `.sph` files to 16kHz `.wav` files using the SoX library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_dir + '/an4/'):\n",
    "    # Untar\n",
    "    tar = tarfile.open(an4_path)\n",
    "    tar.extractall(path=data_dir)\n",
    "    print(\"Completed untarring the an4 tarfile\")\n",
    "    # Convert .sph to .wav (using sox)\n",
    "    print(\"Converting .sph to .wav...\")\n",
    "    sph_list = glob.glob(data_dir + '/an4/**/*.sph', recursive=True)\n",
    "    for sph_path in sph_list:\n",
    "        wav_path = sph_path[:-4] + '.wav'\n",
    "        #converting to 16kHz wav\n",
    "        cmd = f\"sox {sph_path} -r 16000 {wav_path}\"\n",
    "        subprocess.call(cmd, shell=True)\n",
    "    print(\"Finished converting the .sph files to .wav files\")\n",
    "else:\n",
    "    print(\"an4 dataset directory already exists. Proceed to the next step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's build the manifest files for the AN4 dataset. The manifest file is a `.json` file that maps the `.wav` clip to its corresponding text.\n",
    "\n",
    "Each entry in the AN4 dataset's manifest `.json` file follows the template:  \n",
    "`{\"audio_filepath\": \"<.wav file location>\", \"duration\": <duration of the .wav file>, \"text\": \"<text from the .wav file>\"}`  \n",
    "Example: `{\"audio_filepath\": \"/tutorials/am_finetuning/an4/wav/an4_clstk/fash/an251-fash-b.wav\", \"duration\": 1.0, \"text\": \"yes\"}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mutagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries.\n",
    "import json\n",
    "from mutagen.wave import WAVE\n",
    "\n",
    "# Method to build a manifest.\n",
    "def build_manifest(transcripts_path, manifest_path, wav_path):\n",
    "    with open(transcripts_path, 'r') as fin:\n",
    "        with open(manifest_path, 'w') as fout:\n",
    "            for line in fin:\n",
    "                # Lines look like this:\n",
    "                # <s> transcript </s> (fileID)\n",
    "                transcript = line[: line.find('(')-1].lower()\n",
    "                transcript = transcript.replace('<s>', '').replace('</s>', '')\n",
    "                transcript = transcript.strip()\n",
    "\n",
    "                file_id = line[line.find('(')+1 : -2]  # e.g. \"cen4-fash-b\"\n",
    "                audio_path = os.path.join(\n",
    "                    data_dir, wav_path,\n",
    "                    file_id[file_id.find('-')+1 : file_id.rfind('-')],\n",
    "                    file_id + '.wav')\n",
    "\n",
    "                duration = WAVE(filename=audio_path).info.length\n",
    "\n",
    "                # Write the metadata to the manifest\n",
    "                metadata = {\n",
    "                    \"audio_filepath\": audio_path,\n",
    "                    \"duration\": duration,\n",
    "                    \"text\": transcript\n",
    "                }\n",
    "                json.dump(metadata, fout)\n",
    "                fout.write('\\n')\n",
    "                \n",
    "# Building the manifest files.\n",
    "print(\"***Building manifest files***\")\n",
    "\n",
    "# Building manifest files for the training data\n",
    "train_transcripts = data_dir + '/an4/etc/an4_train.transcription'\n",
    "train_manifest = data_dir + '/an4/train_manifest.json'\n",
    "if not os.path.isfile(train_manifest):\n",
    "    build_manifest(train_transcripts, train_manifest, 'an4/wav/an4_clstk')\n",
    "    print(\"Training manifest created at\", train_manifest)\n",
    "\n",
    "# Building manifest files for the test data\n",
    "test_transcripts = data_dir + '/an4/etc/an4_test.transcription'\n",
    "test_manifest = data_dir + '/an4/test_manifest.json'\n",
    "if not os.path.isfile(test_manifest):\n",
    "    build_manifest(test_transcripts, test_manifest, 'an4/wav/an4test_clstk')\n",
    "    print(\"Test manifest created at\", test_manifest)\n",
    "\n",
    "print(\"***Done***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and process the background noise dataset\n",
    "\n",
    "For background noise, we will use the background noise samples from the Room Impulse Response and Noise database from the openslr database. For each 30 second isotropic noise sample in the dataset we use the first 15 seconds for training and the last 15 seconds for evaluation.\n",
    "\n",
    "Let's first download this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the background noise dataset if it doesn't already exist in `data_dir`. \n",
    "# This will take a few moments...\n",
    "# We also set `noise_path` which points to the downloaded background noise dataset.\n",
    "\n",
    "if not os.path.exists(data_dir + '/rirs_noises.zip'):\n",
    "    slr28_url = 'https://www.openslr.org/resources/28/rirs_noises.zip'\n",
    "    noise_path = wget.download(slr28_url, data_dir)\n",
    "    print(\"Background noise dataset download complete.\")\n",
    "else:\n",
    "    print(\"Background noise dataset already exists. Please proceed to the next step.\")\n",
    "    noise_path = data_dir + '/rirs_noises.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to unzip the `.zip` file, which gives us the dataset audio files as 8-channel `.wav` files, sampled at 16kHz. The format and sample rate suit our purposes, but we need to convert these files to mono-channel to match the files in the AN4 dataset. Fortunately, the SoX library provides tools for that as well. \n",
    "\n",
    "Note: The conversion will take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract noise data\n",
    "from zipfile import ZipFile\n",
    "if not os.path.exists(data_dir + '/RIRS_NOISES'):\n",
    "    try:\n",
    "        with ZipFile(noise_path, \"r\") as zipObj:\n",
    "            zipObj.extractall(data_dir)\n",
    "            print(\"Extracting noise data complete\")\n",
    "        # Convert 8-channel audio files to mono-channel\n",
    "        wav_list = glob.glob(data_dir + '/RIRS_NOISES/**/*.wav', recursive=True)\n",
    "        for wav_path in wav_list:\n",
    "            mono_wav_path = wav_path[:-4] + '_mono.wav'\n",
    "            cmd = f\"sox {wav_path} {mono_wav_path} remix 1\"\n",
    "            subprocess.call(cmd, shell=True)\n",
    "        print(\"Finished converting the 8-channel noise data .wav files to mono-channel\")\n",
    "    except Exception:\n",
    "        print(\"Not extracting. Extracted noise data might already exist.\")\n",
    "else: \n",
    "    print(\"Extracted noise data already exists. Please proceed to the next step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's build the manifest files for the noise data. The manifest file is a `.json` file that maps the `.wav` clip to its corresponding text.\n",
    "\n",
    "Each entry in the noise data's manifest `.json` file follows the template:  \n",
    "`{\"audio_filepath\": \"<.wav file location>\", \"duration\": <duration of the .wav file>, \"offset\": <offset value>, \"text\": \"-\"}`  \n",
    "Example: `{\"audio_filepath\": \"/tutorials/am_finetuning/RIRS_NOISES/real_rirs_isotropic_noises/RVB2014_type1_noise_largeroom1_1_mono.wav\", \"duration\": 30.0, \"offset\": 0, \"text\": \"-\"}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "iso_path = os.path.join(data_dir,\"RIRS_NOISES/real_rirs_isotropic_noises\")\n",
    "iso_noise_list = os.path.join(iso_path, \"noise_list\")\n",
    "\n",
    "# Edit the noise_list file so that it lists the *_mono.wav files instead of the original *.wav files\n",
    "with open(iso_noise_list) as f:\n",
    "    if '_mono.wav' in f.read():\n",
    "        print(f\"{iso_noise_list} has already been processed\")\n",
    "    else:\n",
    "        cmd = f\"sed -i 's|.wav|_mono.wav|g' {iso_noise_list}\"\n",
    "        subprocess.call(cmd, shell=True)\n",
    "        print(f\"Finished processing {iso_noise_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the manifest files from noise files\n",
    "def process_row(row, offset, duration):\n",
    "  try:\n",
    "    entry = {}\n",
    "    wav_f = row['wav_filename']\n",
    "    newfile = wav_f\n",
    "    duration = subprocess.check_output('soxi -D {0}'.format(newfile), shell=True)\n",
    "    entry['audio_filepath'] = newfile\n",
    "    entry['duration'] = float(duration)\n",
    "    entry['offset'] = offset\n",
    "    entry['text'] = row['transcript']\n",
    "    return entry\n",
    "  except Exception as e:\n",
    "    wav_f = row['wav_filename']\n",
    "    newfile = wav_f\n",
    "    print(f\"Error processing {newfile} file!!!\")\n",
    "    \n",
    "train_rows = []\n",
    "test_rows = []\n",
    "\n",
    "with open(iso_noise_list,\"r\") as in_f:\n",
    "    for line in in_f:\n",
    "        row = {}\n",
    "        data = line.rstrip().split()\n",
    "        row['wav_filename'] = os.path.join(data_dir,data[-1])\n",
    "        row['transcript'] = \"-\"\n",
    "        train_rows.append(process_row(row, 0 , 15))\n",
    "        test_rows.append(process_row(row, 15 , 15))\n",
    "\n",
    "# Writing manifest files\n",
    "def write_manifest(manifest_file, manifest_lines):\n",
    "    with open(manifest_file, 'w') as fout:\n",
    "      for m in manifest_lines:\n",
    "        fout.write(json.dumps(m) + '\\n')\n",
    "      print(\"Writing manifest file to\", manifest_file, \"complete\")\n",
    "\n",
    "# Writing training and test manifest files\n",
    "test_noise_manifest = os.path.join(data_dir, \"test_noise.json\")\n",
    "train_noise_manifest = os.path.join(data_dir, \"train_noise.json\")\n",
    "if not os.path.exists(test_noise_manifest):\n",
    "    write_manifest(test_noise_manifest, test_rows)\n",
    "else:\n",
    "    print('Test noise manifest file already exists. Please proceed to the next step.')\n",
    "if not os.path.exists(train_noise_manifest):\n",
    "    write_manifest(train_noise_manifest, train_rows)\n",
    "else:\n",
    "    print('Train noise manifest file already exists. Please proceed to the next step.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the noise-augmented dataset\n",
    "\n",
    "Finally, let's create a noise-augmented dataset by adding noise to the the AN4 dataset with the `add_noise.py` NeMo script. This script generates the noise-augmented audio clips as well as the manifest files. \n",
    "\n",
    "Each entry in the noise-augmented data's manifest file follows the template:  \n",
    "`{\"audio_filepath\": \"<.wav file location>\", \"duration\": <duration of the .wav file>, \"text\": \"<text from the .wav file>\"}`\n",
    "Example: `{\"audio_filepath\": \"/tutorials/am_finetuning/noise_data/train_manifest/train_noise_0db/an251-fash-b.wav\", \"duration\": 1.0, \"text\": \"yes\"}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup:\n",
    "\n",
    "Configure and run the NeMo Docker container. We'll keep it active for the remainder of the data preprocessing section, so as to avoid having to update it with the latest version of the NeMo repo every time we need the container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nemo_container = '<add container name>'\n",
    "nemo_container = 'nvcr.io/nvidia/nemo:22.08'\n",
    "# Don't use --rm flag \n",
    "# Using the --user=$(id -u):$(id -g) flag appears to cause the following error: \n",
    "# ERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: '/.local'\n",
    "# Check the permissions.\n",
    "! docker run --gpus=all -it -d -v $data_dir:$data_dir --net=host --ipc=host \\\n",
    "    --ulimit memlock=-1 --ulimit stack=67108864 --name=nemo_container $nemo_container bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that the container is running the latest version of NeMo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRANCH = 'main'\n",
    "run = f\"python -m pip install git+https://github.com/NVIDIA/NeMo.git@{BRANCH}#egg=nemo_toolkit[all]\"\n",
    "# --user=$(id -u):$(id -g) \n",
    "! docker exec -it nemo_container $run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training dataset\n",
    "Let's create a noise-augmented training dataset using the AN4 training dataset. We'll add noise at different SNRs (Signal-to-Noise Ratios) ranging from 0 to 15 dB SNR using a NeMo script. Note that a 0 dB SNR means that the noise and signal in the given audio file are of equal volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_dir = data_dir + '/noise_data'\n",
    "\n",
    "run = f\"python /workspace/nemo/scripts/dataset_processing/add_noise.py \\\n",
    "    --input_manifest={train_manifest} \\\n",
    "    --noise_manifest={train_noise_manifest} \\\n",
    "    --snrs 0 5 10 15 \\\n",
    "    --out_dir={final_data_dir}\"\n",
    "\n",
    "! docker exec -it nemo_container $run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above script generates a `.json` manifest file each for every SNR value, i.e., one manifest file each for 0, 5, 10 and 15db SNR.  \n",
    "Let's combine all the manifests into a single manifest for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = f\"cat {final_data_dir}/manifests/train* >{final_data_dir}/manifests/noisy_train.json\"\n",
    "!{run}\n",
    "\n",
    "print(\"Noise-augmented training dataset created at\", final_data_dir + \"/manifests/noisy_train.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test dataset\n",
    "\n",
    "Let's create a noise-augmented evaluation dataset using the an4 test dataset, by adding noise at 5 dB, using a NeMo script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmention - Add noise to test set.\n",
    "run = f\"python /workspace/nemo/scripts/dataset_processing/add_noise.py \\\n",
    "    --input_manifest={test_manifest} \\\n",
    "    --noise_manifest={test_noise_manifest} \\\n",
    "    --snrs=5 \\\n",
    "    --out_dir={final_data_dir}\"\n",
    "\n",
    "# For some reason, adding --user=$(id -u):$(id -g) breaks the script\n",
    "! docker exec -it nemo_container $run\n",
    "\n",
    "print(\"Noise-augmented testing dataset created at\", final_data_dir+\"/test_manifest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Noise-augmented training manifest and data are created at `{working_dir}/noise_data/noisy_train.json` and `{working_dir}/noise_data/train_manifest` respectively.**  \n",
    "**Noise-augmented testing manifest and data are created at `{working_dir}/noise_data/manifests/test_manifest_test_noise_5db.json` and `{working_dir}/noise_data/test_manifest` respectively.**  \n",
    "\n",
    "With that, step 1 of 3, the data preprocessing step, is complete.  \n",
    "\n",
    "Now onto the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We don't need the NeMo container anymore, so let's get rid of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker container stop nemo_container\n",
    "! docker container rm nemo_container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Fine-tuning the Conformer-CTC model with TAO.\n",
    "Proceed to [this tutorial](https://github.com/nvidia-riva/tutorials/blob/stable/asr-python-advanced-finetune-am-conformer-ctc-tao-finetuning.ipynb) to fine-tune the Conformer-CTC model with TAO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing and Setting up TAO\n",
    "\n",
    "Install TAO inside a Python virtual environment. We recommend performing this step first and then launching the tutorial from the virtual environment.\n",
    "\n",
    "In addition to installing the TAO Python package, ensure you meet the following software requirements:\n",
    "\n",
    "1. `python` >= 3.6.9\n",
    "2. `docker-ce` > 19.03.5\n",
    "3. `docker-API` 1.40\n",
    "4. `nvidia-container-toolkit` > 1.3.0\n",
    "5. `nvidia-container-runtime` > 3.4.0\n",
    "6. `nvidia-docker2` > 2.5.0\n",
    "7. `nvidia-driver` >= 455.23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing TAO is a simple `pip` install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nvidia-pyindex\n",
    "! pip install nvidia-tao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please define these paths on your local host machine\n",
    "HOST_DATA_DIR = '/path/to/your/host/data'\n",
    "HOST_SPECS_DIR = '/path/to/your/host/specs'\n",
    "HOST_RESULTS_DIR = '/path/to/your/host/results'\n",
    "\n",
    "%env HOST_DATA_DIR=$HOST_DATA_DIR\n",
    "%env HOST_SPECS_DIR=$HOST_SPECS_DIR\n",
    "%env HOST_RESULTS_DIR=$HOST_RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p $HOST_DATA_DIR\n",
    "! mkdir -p $HOST_SPECS_DIR\n",
    "! mkdir -p $HOST_RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the Local Directories to the TAO Docker.\n",
    "import json\n",
    "import os\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "tlt_configs = {\n",
    "   \"Mounts\":[\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_DATA_DIR\"],\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_SPECS_DIR\"],\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_RESULTS_DIR\"],\n",
    "           \"destination\": \"/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.path.expanduser(\"~/.cache\"),\n",
    "           \"destination\": \"/root/.cache\"\n",
    "       }\n",
    "   ],\n",
    "   \"DockerOptions\": {\n",
    "       \"shm_size\": \"16G\", \n",
    "       \"ulimits\": {\n",
    "           \"memlock\": -1,\n",
    "           \"stack\": 67108864\n",
    "       }\n",
    "   }\n",
    "}\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(tlt_configs, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the Docker image versions and the tasks that it performs. You can also check by issuing `tao --help` or:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tao info --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Relevant Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TAO Docker.\n",
    "\n",
    "# The data is saved here:\n",
    "DATA_DIR = \"/data\"\n",
    "SPECS_DIR = \"/specs\"\n",
    "RESULTS_DIR = \"/results\"\n",
    "\n",
    "# Set your encryption key and use the same key for all commands.\n",
    "KEY = 'tlt_encode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command structure for the TAO interface can be broken down as follows: `tao <task name> <subcommand>` <br> \n",
    "\n",
    "Let's see this in further detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make copies of the noisy data manifest files for use with TAO\n",
    "\n",
    "The paths in the manifest files are currently set w.r.t. the host system running this notebook. TAO requires manifest files with paths set w.r.t. the interior of the TAO container. Thus, we'll make copies and edit them accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd $HOST_DATA_DIR/noise_data/manifests\n",
    "cp noisy_train.json tao_noisy_train.json\n",
    "cp test_manifest_test_noise_5db.json tao_noisy_test_5db.json\n",
    "sed -i \"s|$HOST_DATA_DIR|/data|g\" tao_noisy_train.json\n",
    "sed -i \"s|$HOST_DATA_DIR|/data|g\" tao_noisy_test_5db.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Downloading Specs\n",
    "TAO's conversational AI toolkit works off of spec files which make it easy to edit hyperparameters on the fly. We can proceed to downloading the spec files. You may choose to modify/rewrite these specs or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command.<br>\n",
    "\n",
    "The `-o` argument indicates the folder where the default specification files will be downloaded. The `-r` argument instructs the script on where to save the logs. **Ensure the `-o` points to an empty folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the specs directory if it is already there to avoid errors\n",
    "! tao speech_to_text_conformer download_specs \\\n",
    "    -r $RESULTS_DIR/conformer \\\n",
    "    -o $SPECS_DIR/conformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is trained, evaluated, and there is a need for fine-tuning, the following command can be used to fine-tune the ASR model. This step can also be used for transfer learning by making changes in the `train.json` and `dev.json` files to add new data.\n",
    "\n",
    "The list for customizations is the same as the training parameters with the exception for parameters which affect the model architecture. Also, instead of `training_ds` we have `finetuning_ds`.\n",
    "\n",
    "Note: If you want to proceed with a trained dataset for better inference results, you can find a `.nemo` model [here](\n",
    "https://ngc.nvidia.com/catalog/collections/nvidia:nemotrainingframework).\n",
    "\n",
    "Simply rename the `.nemo` file to `.tlt` and pass it through the fine-tune pipeline.\n",
    "\n",
    "Note: The fine-tune spec files contain specifics to fine-tune the English model we just trained to Russian. If you want to proceed with English, ensure the changes are in the spec file `finetune.yaml` which you can find in the `SPEC_DIR` folder you mapped. Ensure to delete older fine-tuning checkpoints if you choose to change the language after fine-tuning it as-is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can conduct the fine-tuning, we need to pre-process the text. This step is called subword tokenization that creates a subword vocabulary for the text. This is different from Jasper/QuartzNet because only single characters are regarded as elements in the vocabulary in their cases, while in Conformer-CTC, the subword can be one or multiple characters. We can use the `create_tokenizer` command to create the tokenizer that generates the subword vocabulary for us for use in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tao speech_to_text_conformer create_tokenizer \\\n",
    "    -e $SPECS_DIR/conformer/create_tokenizer.yaml \\\n",
    "    -r $RESULTS_DIR/conformer_noisy_audio/tokenizer \\\n",
    "    manifests=$DATA_DIR/noise_data/manifests/noisy_train.json \\\n",
    "    output_root=$RESULTS_DIR/conformer_noisy_audio/tokenizer \\\n",
    "    vocab_size=1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download a Pre-Trained Conformer-CTC Acoustic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd $HOST_RESULTS_DIR\n",
    "mkdir -p conformer_noisy_audio/pretrained\n",
    "ngc registry model download-version \"nvidia/tao/speechtotext_en_us_conformer:trainable_v4.0\"\n",
    "mv speechtotext_en_us_conformer_vtrainable_v4.0/* conformer_noisy_audio/pretrained/.\n",
    "rmdir speechtotext_en_us_conformer_vtrainable_v4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a Fine-Tuning Spec File for the Noisy Audio Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment: `change_vocabulary: false`, no tokenizer in `finetune` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tee $HOST_SPECS_DIR/conformer/finetune_noisy_audio.yaml <<'EOF'\n",
    "# Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.\n",
    "# TLT spec file for fine-tuning a previously trained ASR model.\n",
    "\n",
    "trainer:\n",
    "  max_epochs: 3   # This is low for demo purposes\n",
    "\n",
    "tlt_checkpoint_interval: 1\n",
    "\n",
    "# Whether or not to change the decoder vocabulary.\n",
    "# Note that this MUST be set if the labels change, e.g. to a different language's character set\n",
    "# or if additional punctuation characters are added.\n",
    "change_vocabulary: false\n",
    "\n",
    "tokenizer:\n",
    "  dir: ???\n",
    "  type: \"bpe\"  # Can be either bpe or wpe\n",
    "\n",
    "# Fine-tuning settings: training dataset\n",
    "finetuning_ds:\n",
    "  manifest_filepath: ???\n",
    "  batch_size: 4\n",
    "  trim_silence: true\n",
    "  shuffle: true\n",
    "  is_tarred: false\n",
    "  tarred_audio_filepaths: null\n",
    "\n",
    "# Fine-tuning settings: validation dataset\n",
    "validation_ds:\n",
    "  manifest_filepath: ???\n",
    "  batch_size: 4\n",
    "  shuffle: false\n",
    "\n",
    "# Fine-tuning settings: optimizer\n",
    "optim:\n",
    "  name: novograd\n",
    "  lr: 0.001\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tune Conformer-CTC\n",
    "Fine-tuning and validating for even a single epoch on the noise-augmented AN4 dataset will take a considerable amount of time. For good model performance, dozens of training epochs may be required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment: `change_vocabulary: false` in `.yaml` file, no tokenizer in `finetune` command\n",
    "I.e., get rid of `tokenizer.dir=$RESULTS_DIR/conformer_noisy_audio/tokenizer/tokenizer_spe_unigram_v1024`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I successfully generated a fine-tuned `.tlt` model with the experiment. Now to check if ONNX inference works. \n",
    "\n",
    "SPOILER ALERT: It didn't. There turned out to be a bug in `tao speech_to_text_conformer infer_onnx`. The entire subtask was removed from the most recent TAO release (as of November 2022). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao speech_to_text_conformer finetune \\\n",
    "     -e $SPECS_DIR/conformer/finetune_noisy_audio.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/conformer_noisy_audio/pretrained/speechtotext_en_us_conformer.tlt \\\n",
    "     -r $RESULTS_DIR/conformer_noisy_audio/finetune \\\n",
    "     finetuning_ds.manifest_filepath=$DATA_DIR/noise_data/manifests/tao_noisy_train.json \\\n",
    "     validation_ds.manifest_filepath=$DATA_DIR/noise_data/manifests/tao_noisy_test_5db.json \\\n",
    "     trainer.max_epochs=25 \\\n",
    "     finetuning_ds.num_workers=20 \\\n",
    "     validation_ds.num_workers=20 \\\n",
    "     trainer.gpus=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fine-tuned our model, we need to check how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaced \n",
    "# -m $RESULTS_DIR/conformer_noisy_audio/finetune/checkpoints/finetuned-model.tlt\n",
    "# with \n",
    "# -m $RESULTS_DIR/conformer_noisy_audio/finetune/finetuned-model_epoch_24.tlt\n",
    "# and later with \n",
    "# -m $RESULTS_DIR/conformer_noisy_audio/finetune/finetuned-model_epoch_9.tlt\n",
    "!tao speech_to_text_conformer evaluate \\\n",
    "     -e $SPECS_DIR/conformer/evaluate.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/conformer_noisy_audio/finetune/finetuned-model_epoch_9.tlt \\\n",
    "     -r $RESULTS_DIR/conformer_noisy_audio/evaluate \\\n",
    "     test_ds.manifest_filepath=$DATA_DIR/noise_data/manifests/tao_noisy_test_5db.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In developing this tutorial, we observed that fine-tuning Conformer-CTC for 10 epochs yielded a WER of approximately 0.52% on our noisy test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Export to Riva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replaced \n",
    "# -m $RESULTS_DIR/conformer_noisy_audio/finetune/checkpoints/finetuned-model.tlt\n",
    "# with \n",
    "# -m $RESULTS_DIR/conformer_noisy_audio/finetune/finetuned-model_epoch_9.tlt\n",
    "!tao speech_to_text_conformer export \\\n",
    "     -e $SPECS_DIR/conformer/export.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/conformer_noisy_audio/finetune/finetuned-model_epoch_9.tlt \\\n",
    "     -r $RESULTS_DIR/conformer_noisy_audio/riva \\\n",
    "     export_format=RIVA \\\n",
    "     export_to=conformer-ctc-noisy-audio.riva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to ONNX\n",
    "Note: Export to ONNX is not needed for Riva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replaced \n",
    "# -m $RESULTS_DIR/conformer_noisy_audio/finetune/checkpoints/finetuned-model.tlt\n",
    "# with \n",
    "# -m $RESULTS_DIR/conformer_noisy_audio/finetune/finetuned-model_epoch_9.tlt\n",
    "!tao speech_to_text_conformer export \\\n",
    "     -e $SPECS_DIR/conformer/export.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/conformer_noisy_audio/finetune/finetuned-model_epoch_9.tlt \\\n",
    "     -r $RESULTS_DIR/conformer_noisy_audio/onnx \\\n",
    "     export_format=ONNX \\\n",
    "     export_to=conformer-ctc-noisy-audio.eonnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR Inference using TLT Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ASR Inference with TAO Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to run inference on the TLT checkpoint with TAO Toolkit. \n",
    " For real-time inference and best latency, we need to deploy this model on Riva. Refer to the [How to Deploy a Custom Acoustic Model (Conformer-CTC) Trained with TAO Toolkit on Riva](https://github.com/nvidia-riva/tutorials/blob/stable/asr-python-advanced-finetune-am-conformer-ctc-tao-deployment.ipynb) tutorial. \n",
    " You might have to work with the `infer.yaml` file to select the files you want for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaced \n",
    "# -m $RESULTS_DIR/conformer_noisy_audio/finetune/checkpoints/finetuned-model.tlt\n",
    "# with \n",
    "# -m $RESULTS_DIR/conformer_noisy_audio/finetune/finetuned-model_epoch_9.tlt\n",
    "!tao speech_to_text_conformer infer \\\n",
    "     -e $SPECS_DIR/conformer/infer.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/conformer_noisy_audio/finetune/finetuned-model_epoch_9.tlt \\\n",
    "     -r $RESULTS_DIR/conformer_noisy_audio/infer \\\n",
    "     file_paths=[$DATA_DIR/noise_data/test_manifest/test_noise_5db/an420-fjlp-b.wav]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Deploying the fine-tuned Conformer-CTC TAO model on the Riva Speech Skills server.\n",
    "Proceed to [this tutorial](https://github.com/nvidia-riva/tutorials/blob/stable/asr-python-advanced-finetune-am-conformer-ctc-tao-deployment.ipynb) to deploy the fine-tuned Conformer-CTC TAO model on the Riva Speech Skills server for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVIDIA Riva Overview\n",
    "\n",
    "NVIDIA Riva is a GPU-accelerated SDK for building speech AI applications that are customized for your use case and deliver real-time performance. <br/>\n",
    "Riva offers a rich set of speech and natural language understanding services such as:\n",
    "\n",
    "- Automated speech recognition (ASR).\n",
    "- Text-to-Speech synthesis (TTS).\n",
    "- A collection of natural language processing (NLP) services, such as named entity recognition (NER), punctuation, and intent classification.\n",
    "\n",
    "In this tutorial, we will deploy a custom acoustic model (Conformer-CTC) trained with TAO Toolkit on Riva. <br> \n",
    "To understand the basics of Riva ASR APIs, refer to [Getting started with Riva ASR in Python](https://github.com/nvidia-riva/tutorials/blob/stable/asr-python-basics.ipynb). <br>\n",
    "\n",
    "For more information about Riva, refer to the [Riva developer documentation](https://developer.nvidia.com/riva)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Adapt, and Optimize TAO Toolkit\n",
    "[Train Adapt Optimize (TAO) Toolkit](https://developer.nvidia.com/tao-toolkit) provides the capability to export your model in a format that can be deployed using [NVIDIA Riva](https://developer.nvidia.com/riva), a highly performant application framework for multi-modal conversational AI services using GPUs. \n",
    "\n",
    "This tutorial explores taking a `.riva` model, the result of the `tao speech_to_text_conformer train` command (refer to the [fine-tuning tutorial](https://github.com/nvidia-riva/tutorials/blob/stable/sven-asr-python-advanced-finetune-am-conformer-ctc-tao-finetuning.ipynb)) and leveraging the Riva ServiceMaker framework to aggregate all the necessary artifacts for Riva deployment to a target environment. After the model is deployed in Riva, you can issue inference requests to the server. We will demonstrate how quick and straightforward this whole process is.\n",
    "In this tutorial, you will learn how to:  \n",
    "- Use Riva ServiceMaker to take a TAO exported `.riva` file and convert it to `.rmir`.\n",
    "- Deploy the model locally on the Riva server.\n",
    "- Send inference requests from a demo client using Riva API bindings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, ensure you have:\n",
    "- Access to NVIDIA NGC and are able to download the Riva Quick Start [resources](https://ngc.nvidia.com/catalog/resources/nvidia:riva:riva_quickstart).\n",
    "- A `.riva` model file that you want to deploy. You can obtain this from `tao <task> export` (with `export_format=RIVA`) or download a pre-trained version from the [US English Conformer NGC model page](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechtotext_en_us_conformer). For more information on training and exporting a `.riva` Conformer-CTC acoustic model, refer to the [Speech Recognition with Conformer](https://docs.nvidia.com/tao/tao-toolkit/text/asr/speech_recognition_with_conformer.html) pages in the [TAO Toolkit Documentation](https://docs.nvidia.com/tao/tao-toolkit/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Riva ServiceMaker\n",
    "Riva ServiceMaker is a set of tools that aggregates all the necessary artifacts (models, files, configurations, and user settings) for Riva deployment to a target environment. It has two main components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riva-Build\n",
    "\n",
    "This step helps build a Riva-ready version of the model. Its only output is an intermediate format (called an RMIR) of an end-to-end pipeline for the supported services within Riva. Let's consider a Conformer-CTC ASR model. <br>\n",
    "\n",
    "`riva-build` is responsible for the combination of one or more exported models (`.riva` files) into a single file containing an intermediate format called Riva Model Intermediate Representation (`.rmir`). This file contains a deployment-agnostic specification of the whole end-to-end pipeline along with all the assets required for the final deployment and inference. For more information, refer to the [documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-pipeline-configuration.html?highlight=pipeline%20configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p models\n",
    "!cp $HOST_RESULTS_DIR/conformer_noisy_audio/riva/conformer-ctc-noisy-audio.riva models/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: UPDATE THESE PATHS \n",
    "\n",
    "# Delete this import statement from the repo version of the notebook\n",
    "import os\n",
    "\n",
    "# ServiceMaker Docker\n",
    "RIVA_SM_CONTAINER = \"<add container name>\"\n",
    "\n",
    "# Directory where the .riva model is stored $MODEL_LOC/*.riva\n",
    "MODEL_LOC = \"<add path to model location>\"\n",
    "\n",
    "# Name of the .riva file\n",
    "MODEL_NAME = \"<add model name>\"\n",
    "\n",
    "# Key that model is encrypted with, while exporting with TAO\n",
    "KEY = \"<add encryption key used for trained model>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ServiceMaker Docker container\n",
    "! docker pull $RIVA_SM_CONTAINER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it doesn't already exist, create a sub-directory inside `MODEL_LOC` to store your `.rmir` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p $MODEL_LOC/rmir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the `.rmir` file.\n",
    "\n",
    "**Notes** \n",
    "1. If you obtained your `.riva`-formatted acoustic model file from `tao <task> export`, you may need to replace `--nn.fp16_needs_obey_precision_pass` with `--nn.use_trt_fp32` when invoking `riva-build`. \n",
    "2. Refer to the [Riva ASR Pipeline Configuration](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-pipeline-configuration.html) documentation page if you wish to build an ASR pipeline for a supported language other than US English. To obtain the proper `riva-build` parameters for your particular application, select the acoustic model, language, and pipeline type (offline for the purposes of this tutorial) from the interactive web menu at the bottom of the first section of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax: riva-build <task-name> output-dir-for-rmir/model.rmir:key dir-for-riva/model.riva:key\n",
    "! docker run --rm --gpus 0 -v $MODEL_LOC:/data $RIVA_SM_CONTAINER -- \\\n",
    "    riva-build speech_recognition \\\n",
    "        /data/rmir/asr_offline_conformer_ctc_noisy_audio.rmir:$KEY \\\n",
    "        /data/$MODEL_NAME:$KEY \\\n",
    "        --offline \\\n",
    "        --name=asr_offline_conformer_ctc_noisy_audio_pipeline \\\n",
    "        --decoder_type=greedy \\\n",
    "        --ms_per_timestep=40 \\\n",
    "        --chunk_size=4.8 \\\n",
    "        --left_padding_size=1.6 \\\n",
    "        --right_padding_size=1.6 \\\n",
    "        --max_batch_size=16 \\\n",
    "        --nn.use_trt_fp32 \\\n",
    "        --featurizer.use_utterance_norm_params=False \\\n",
    "        --featurizer.precalc_norm_time_steps=0 \\\n",
    "        --featurizer.precalc_norm_params=False \\\n",
    "        --featurizer.max_batch_size=512 \\\n",
    "        --featurizer.max_execution_batch_size=512 \\\n",
    "        --language_code=en-US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riva-Deploy\n",
    "\n",
    "The deployment tool takes as input one or more RMIR files and a target model repository directory. It creates an ensemble configuration specifying the pipeline for the execution and finally writes all those assets to the output model repository directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax: riva-deploy -f dir-for-rmir/model.rmir:key output-dir-for-repository\n",
    "! docker run --rm --gpus 0 -v $MODEL_LOC:/data $RIVA_SM_CONTAINER -- \\\n",
    "    riva-deploy -f  \\\n",
    "        /data/rmir/asr_offline_conformer_ctc_noisy_audio.rmir:$KEY \\\n",
    "        /data/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Start the Riva Server\n",
    "After the model repository is generated, we are ready to start the Riva server. First, download the Riva Quick Start resource from NGC. \n",
    "Set the path to the directory here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Riva Quick Start directory\n",
    "RIVA_DIR = \"<Path to the uncompressed folder downloaded from quickstart(include the folder name)>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we modify the `config.sh` file to enable the relevant Riva services (ASR for the Conformer-CTC model), provide the encryption key, and path to the model repository (`riva_model_loc`) generated in the previous step among other configurations. \n",
    "\n",
    "For example, if above the model repository is generated at `$MODEL_LOC/models`, then you can specify `riva_model_loc` as the same directory as `MODEL_LOC`. <br>\n",
    "\n",
    "Pretrained versions of models specified in `models_asr/nlp/tts` are fetched from NGC. Since we are using our custom model, we can comment it in `models_asr` (and any others that are not relevant to your use case). <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### config.sh snippet\n",
    "```\n",
    "# Enable or Disable Riva Services \n",
    "service_enabled_asr=true                                                      ## MAKE CHANGES HERE\n",
    "service_enabled_nlp=false                                                      ## MAKE CHANGES HERE\n",
    "service_enabled_tts=false                                                     ## MAKE CHANGES HERE\n",
    "\n",
    "# Specify one or more GPUs to use\n",
    "# specifying more than one GPU is currently an experimental feature, and may result in undefined behaviours.\n",
    "gpus_to_use=\"device=0\"\n",
    "\n",
    "# Specify the encryption key to use to deploy models\n",
    "MODEL_DEPLOY_KEY=\"tlt_encode\"                                                  ## MAKE CHANGES HERE\n",
    "\n",
    "# Locations to use for storing models artifacts\n",
    "#\n",
    "# If an absolute path is specified, the data will be written to that location\n",
    "# Otherwise, a docker volume will be used (default).\n",
    "#\n",
    "# riva_init.sh will create a `rmir` and `models` directory in the volume or\n",
    "# path specified. \n",
    "#\n",
    "# RMIR ($riva_model_loc/rmir)\n",
    "# Riva uses an intermediate representation (RMIR) for models\n",
    "# that are ready to deploy but not yet fully optimized for deployment. Pretrained\n",
    "# versions can be obtained from NGC (by specifying NGC models below) and will be\n",
    "# downloaded to $riva_model_loc/rmir by `riva_init.sh`\n",
    "# \n",
    "# Custom models produced by NeMo or TAO and prepared using riva-build\n",
    "# may also be copied manually to this location $(riva_model_loc/rmir).\n",
    "#\n",
    "# Models ($riva_model_loc/models)\n",
    "# During the riva_init process, the RMIR files in $riva_model_loc/rmir\n",
    "# are inspected and optimized for deployment. The optimized versions are\n",
    "# stored in $riva_model_loc/models. The riva server exclusively uses these\n",
    "# optimized versions.\n",
    "riva_model_loc=\"<add path>\"                              ## MAKE CHANGES HERE (Replace with MODEL_LOC)                      \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have permission to execute these scripts\n",
    "! cd $RIVA_DIR && chmod +x ./riva_init.sh && chmod +x ./riva_start.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Riva Init. This will fetch the containers/models\n",
    "# YOU CAN SKIP THIS STEP IF YOU DID RIVA DEPLOY\n",
    "# ! cd $RIVA_DIR && ./riva_init.sh config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Riva Start. This will deploy your model(s).\n",
    "! cd $RIVA_DIR && ./riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Inference\n",
    "After the Riva server is up and running with your models, you can send inference requests querying the server. \n",
    "\n",
    "To send gRPC requests, install the Riva Python API bindings for the client. This is available as a `pip` `.whl` file with the Quick Start.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Client API Bindings\n",
    "! pip install nvidia-riva-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the Riva Server and Run Inference\n",
    "Now we can actually query the Riva server. The following cell queries the Riva server (using gRPC) to yield a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import grpc\n",
    "import time\n",
    "import riva.client\n",
    "import wave\n",
    "\n",
    "# audio_file = \"<add path to .wav file>\"\n",
    "audio_file = os.path.join(HOST_DATA_DIR, \"noise_data/test_manifest/test_noise_5db/an420-fjlp-b.wav\")\n",
    "server = \"localhost:50051\"\n",
    "\n",
    "with open(audio_file, 'rb') as fh:\n",
    "    data = fh.read()\n",
    "\n",
    "auth = riva.client.Auth(uri=server)\n",
    "client = riva.client.ASRService(auth)\n",
    "config = riva.client.RecognitionConfig(\n",
    "    encoding=riva.client.AudioEncoding.LINEAR_PCM,\n",
    "    language_code=\"en-US\",\n",
    "    max_alternatives=1,\n",
    "    enable_automatic_punctuation=False,\n",
    ")\n",
    "riva.client.add_audio_file_specs_to_config(config, audio_file)\n",
    "\n",
    "response = client.offline_recognize(data, config)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the Riva Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd $RIVA_DIR && bash riva_stop.sh"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ASR_with_NeMo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "venv-riva-tutorials",
   "language": "python",
   "name": "venv-riva-tutorials"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
