{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/riva_asr_asr-python-advanced-finetune-am-citrinet-tao-finetuning/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to fine-tune a Riva ASR Acoustic Model (Citrinet) with TAO Toolkit\n",
    "This tutorial walks you through how to fine-tune a Riva ASR acoustic model (Citrinet) with TAO Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this tutorial, we are going to discuss the Citrinet model, which is an end-to-end ASR model that takes in audio and produces text.\n",
    "\n",
    "Citrinet is a descendent of QuartzNet that features the squeeze-and-excitation (SE) block and sub-word tokenization and has a better accuracy/performance than QuartzNet.\n",
    "\n",
    "![CitriNet with CTC](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/_images/citrinet_vertical.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ASR using TAO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TAO launcher uses Docker containers under the hood, and **for our data and results directory to be visible to Docker, they need to be mapped**. The launcher can be configured using the config file `~/.tao_mounts.json`. Apart from the mounts, you can also configure additional options like the environment variables and the amount of shared memory available to the TAO launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The following code creates a sample `~/.tao_mounts.json`  file. Here, we can map directories in which we save the data, specs, results, and cache. You should configure it for your specific use case so these directories are correctly visible to the Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directory for this tutorial\n",
    "WORKING_DIR = 'asr_am_finetuning'\n",
    "\n",
    "# Defining paths on the local host machine\n",
    "%env HOST_DATA_DIR = {WORKING_DIR}/data\n",
    "%env HOST_SPECS_DIR = {WORKING_DIR}/specs\n",
    "%env HOST_RESULTS_DIR = {WORKING_DIR}/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating directories on the local host machine\n",
    "! mkdir -p $WORKING_DIR\n",
    "! mkdir -p $HOST_DATA_DIR\n",
    "! mkdir -p $HOST_SPECS_DIR\n",
    "! mkdir -p $HOST_RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "import os\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "tao_configs = {\n",
    "   \"Mounts\":[\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_DATA_DIR\"],\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_SPECS_DIR\"],\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_RESULTS_DIR\"],\n",
    "           \"destination\": \"/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.path.expanduser(\"~/.cache\"),\n",
    "           \"destination\": \"/root/.cache\"\n",
    "       }\n",
    "   ],\n",
    "   \"DockerOptions\": {\n",
    "        \"shm_size\": \"128G\",\n",
    "        \"ulimits\": {\n",
    "            \"memlock\": -1,\n",
    "            \"stack\": 67108864\n",
    "         }\n",
    "   }\n",
    "}\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(tao_configs, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the Docker image versions and the tasks that it performs. You can also check by issuing `tao --help` or:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tao info --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Relevant Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TAO Docker.\n",
    "\n",
    "# The data is saved here:\n",
    "DATA_DIR = \"/data\"\n",
    "SPECS_DIR = \"/specs\"\n",
    "RESULTS_DIR = \"/results\"\n",
    "\n",
    "# Set the encryption key and use the same key for all commands.\n",
    "KEY = 'tlt_encode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command structure for the TAO interface can be broken down as follows: `tao <task name> <subcommand>` <br> \n",
    "\n",
    "Let's see this in further detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Downloading Specs\n",
    "TAO's conversational AI toolkit works off of spec files which make it easy to edit hyperparameters on the fly. We can proceed to downloading the spec files. You may choose to modify/rewrite these specs or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command.<br>\n",
    "\n",
    "The `-o` argument indicates the folder where the default specification files will be downloaded. The `-r` argument instructs the script on where to save the logs. **Ensure the `-o` points to an empty folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the specs directory if it is already there to avoid errors\n",
    "! tao speech_to_text_citrinet download_specs \\\n",
    "    -r $RESULTS_DIR/speech_to_text_citrinet \\\n",
    "    -o $SPECS_DIR/speech_to_text_citrinet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will use the popular AN4 dataset. This is a small dataset recorded and distributed by Carnegie Mellon University. It consists of recordings of people spelling out addresses, names, etc. Information about this dataset can be found on the [official CMU site](http://www.speech.cs.cmu.edu/databases/an4/).\n",
    "\n",
    "Let's download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the dataset exists, otherwise download it\n",
    "if os.path.exists(os.environ[\"HOST_DATA_DIR\"] + '/an4_sphere.tar.gz'):\n",
    "    print(\"Dataset exists, skipping download\")\n",
    "else:\n",
    "    print(\"Dataset does not exist, downloading\")\n",
    "    !wget https://dldata-public.s3.us-east-2.amazonaws.com/an4_sphere.tar.gz -P $DATA_DOWNLOAD_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untar the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! tar -xf $HOST_DATA_DIR/an4_sphere.tar.gz -C $HOST_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The an4 dataset contains transcripts in the format `<s> transcript </s> (fileID)`, where:\n",
    "1. `<s>` - denotes the start of the transcript\n",
    "2. `</s>` - denotes the end of the transcript\n",
    "3. `(fileID)` - denotes the name of the .wav file corresponding to this transcript\n",
    "\n",
    "The audio files are in `.sph` format and the dataset is already split into train and test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tao speech_to_text_citrinet dataset_convert` command processes the input dataset (in MCV or AN4 format) and converts to TAO/NeMo format.\n",
    "\n",
    "In TAO/NeMo format, the dataset consists of a set of utterances in individual audio files (.wav) and a manifest that describes the dataset, with information about one utterance per line.<br>\n",
    "Each line of the manifest should be in the following format:\n",
    "\n",
    "{\"audio_filepath\": \"/path/to/audio.wav\", \"text\": \"the transcription of the utterance\", \"duration\": 23.147}\n",
    "\n",
    "The `audio_filepath` field should provide an absolute path to the .wav file corresponding to the utterance. The `text` field should contain the full transcript for the utterance, and the `duration` field should reflect the duration of the utterance in seconds.\n",
    "\n",
    "We will run the following command, specifying the format of the input dataset in `dataset_convert_an4.yaml` and the path to the dataset through `source_data_dir` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! tao speech_to_text_citrinet dataset_convert \\\n",
    "    -e $SPECS_DIR/speech_to_text_citrinet/dataset_convert_an4.yaml \\\n",
    "    -r $RESULTS_DIR/citrinet/dataset_convert \\\n",
    "    source_data_dir=$DATA_DIR/an4 \\\n",
    "    target_data_dir=$DATA_DIR/an4_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's listen to a sample audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change path of the file here to listen to some other audio\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "path = os.environ[\"HOST_DATA_DIR\"] + '/an4_converted/wavs/an268-mbmg-b.wav'\n",
    "ipd.Audio(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Finetuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can do the actual finetuning, we will tokenize the text.\n",
    "TAO provides implementation of 2 SubWord tokenization techniques - WordPiece (WP) and SentencePiece (SP).<br>\n",
    "For SentencePiece, TAO also provides the option to select between these different types - unigram, bpe, char & word.\n",
    "\n",
    "Subword tokenization creates a subword vocabulary for the text. The core concept behind subwords is that frequently occurring words should be in the vocabulary, whereas rare words should be split into frequent sub words. Eg. The word “refactoring” can be split into “re”, “factor”, and “ing”. \n",
    "\n",
    "For training Citrinet, we use the `create_tokenizer` command to create the tokenizer that generates the unigram SP subword vocabulary. <br>\n",
    "The `create_tokenizer.yaml` contains the following specifications for tokenization:\n",
    "```\n",
    "vocab_size: 1024\n",
    "tokenizer:\n",
    "    tokenizer_type: \"spe\"\n",
    "    spe_type: \"unigram\"\n",
    "    spe_character_coverage: 1.0\n",
    "    lower_case: False\n",
    "```\n",
    "\n",
    "BPE relies on a pre-tokenizer that splits the training data into words. Pretokenization can be as simple as space tokenization.<br>\n",
    "WordPiece is similar to BPE since it includes all the characters and symbols into its base vocabulary first. BPE and WordPiece lies in the way the symbol pairs are chosen for adding to the vocabulary.<br>\n",
    "Unigram tokenization also starts with setting a desired vocabulary size. However, the main difference between unigram and the previous 2 approaches is that we don’t start with a base vocabulary of characters only. Instead, the base vocabulary has all the words and symbols. <br>\n",
    "All the tokenizers above assume that space separates words. This is true except for a few languages like Chinese, Japanese etc. SentencePiece does not treat space as a separator, instead, it takes the string as input in its original raw format, i.e. along with all spaces. It then uses BPE or unigram as its tokenizers to construct the vocabulary.\n",
    "\n",
    "Feel free to read [HuggingFace's blog](https://huggingface.co/docs/transformers/tokenizer_summary) to learn more about tokenization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao speech_to_text_citrinet create_tokenizer \\\n",
    "-e $SPECS_DIR/speech_to_text_citrinet/create_tokenizer.yaml \\\n",
    "-r $RESULTS_DIR/citrinet/create_tokenizer \\\n",
    "manifests=$DATA_DIR/an4_converted/train_manifest.json \\\n",
    "output_root=$DATA_DIR/an4 \\\n",
    "vocab_size=32 # to create an apt vocab for accoustic model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data and the tokenizer ready, let's download the pre-trained Citrinet checkpoint that we will use for finetuning. We will download the ASR model, [Citrinet-1024](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechtotext_en_us_citrinet), that is used in Riva ASR Speech skill.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the checkpoint exists, otherwise download it\n",
    "if os.path.exists(os.environ[\"HOST_RESULTS_DIR\"] + '/speechtotext_en_us_citrinet_vtrainable_v3.0/'):\n",
    "    print(\"Checkpoint exists, skipping download\")\n",
    "else:\n",
    "    print(\"Checkpoint does not exist, downloading\")\n",
    "    ! ngc registry model download-version \"nvidia/tao/speechtotext_en_us_citrinet:trainable_v3.0\"\n",
    "    ! mv speechtotext_en_us_citrinet_vtrainable_v3.0/ $HOST_RESULTS_DIR/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The fine-tune spec file (`$SPECS_DIR/finetune.yaml`) contain specifics to fine-tune the English AM model, that we just downloaded, to Russian language (also called as language adaptation). In order to fine-tune the model for English language (an4 is an English ASR dataset), we will modify that spec file.\n",
    "\n",
    "Here is the minimal spec file that we will use for finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile finetune_en.yaml\n",
    "\n",
    "# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n",
    "# TLT spec file for fine-tuning a previously trained ASR models based on CTC over the MCV Russian dataset.\n",
    "\n",
    "trainer:\n",
    "  max_epochs: 3   # This is low for demo purposes\n",
    "\n",
    "tlt_checkpoint_interval: 1\n",
    "\n",
    "# Whether or not to change the decoder vocabulary.\n",
    "# Note that this MUST be set if the labels change, e.g. to a different language's character set\n",
    "# or if additional punctuation characters are added.\n",
    "change_vocabulary: false   # CHANGED TO FALSE\n",
    "\n",
    "tokenizer:\n",
    "  dir: ???\n",
    "  type: \"bpe\"  # Can be either bpe or wpe\n",
    "\n",
    "# Fine-tuning settings: training dataset\n",
    "finetuning_ds:\n",
    "  manifest_filepath: ???\n",
    "  sample_rate: 16000\n",
    "  batch_size: 32\n",
    "  trim_silence: true\n",
    "  max_duration: 16.7\n",
    "  shuffle: true\n",
    "  is_tarred: false\n",
    "  tarred_audio_filepaths: null\n",
    "\n",
    "# Fine-tuning settings: validation dataset\n",
    "validation_ds:\n",
    "  manifest_filepath: ???\n",
    "  sample_rate: 16000\n",
    "  batch_size: 32\n",
    "  shuffle: false\n",
    "\n",
    "# Fine-tuning settings: optimizer\n",
    "optim:\n",
    "  name: novograd\n",
    "  lr: 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving the above created specs file\n",
    "!mv finetune_en.yaml $HOST_SPECS_DIR/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For finetuning an ASR Citrinet model in TAO, we use the `tao speech_to_text_citrinet finetune` command with the following arguments:\n",
    "<ul>\n",
    "    <li>`-e`: Path to the spec file </li>\n",
    "    <li>`-g`: Number of GPUs to use </li>\n",
    "    <li>`-r`: Path to the results folder </li>\n",
    "    <li>`-m`: Path to the model </li>\n",
    "    <li>`-k`: User specified encryption key to use while saving/loading the model </li>\n",
    "    <li>Any overrides to the spec file. For example, `trainer.max_epochs`. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao speech_to_text_citrinet finetune \\\n",
    "     -e $SPECS_DIR/finetune_en.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/speechtotext_en_us_citrinet_vtrainable_v3.0/speechtotext_en_us_citrinet.tlt \\\n",
    "     -r $RESULTS_DIR/citrinet/finetune \\\n",
    "     finetuning_ds.manifest_filepath=$DATA_DIR/an4_converted/train_manifest.json \\\n",
    "     validation_ds.manifest_filepath=$DATA_DIR/an4_converted/test_manifest.json \\\n",
    "     trainer.max_epochs=5 \\\n",
    "     finetuning_ds.num_workers=20 \\\n",
    "     validation_ds.num_workers=20 \\\n",
    "     tokenizer.dir=$DATA_DIR/an4/tokenizer_spe_unigram_v32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ASR evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model trained, we need to check how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao speech_to_text_citrinet evaluate \\\n",
    "     -e $SPECS_DIR/speech_to_text_citrinet/evaluate.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/citrinet/finetune/checkpoints/finetuned-model.tlt \\\n",
    "     -r $RESULTS_DIR/citrinet/evaluate \\\n",
    "     test_ds.manifest_filepath=$DATA_DIR/an4_converted/test_manifest.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will observe that the model scores a ~1.94 word error rate (WER) on the an4 test dataset.<br>\n",
    "\n",
    "Word Error Rate is a measure of how accurate an ASR system performs. Quite literally, it calculates how many “errors” are in the transcription text produced by an ASR system, compared to a human transcription. The lower the number, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ASR model export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With TAO, you can also export your model in a format that can deployed using NVIDIA Riva; a highly performant application framework for multi-modal conversational AI services using GPUs. The same command for exporting to ONNX can be used here. The only small variation is the configuration for `export_format` in the spec file.\n",
    "\n",
    "This exported .riva model will be used in the next notebook for deploying the ASR pipeline with this customized accoustic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to Riva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao speech_to_text_citrinet export \\\n",
    "     -e $SPECS_DIR/speech_to_text_citrinet/export.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/citrinet/finetune/checkpoints/finetuned-model.tlt \\\n",
    "     -r $RESULTS_DIR/citrinet/riva \\\n",
    "     export_format=RIVA \\\n",
    "     export_to=asr-model.riva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to ONNX (Note: Export to ONNX is not needed for Riva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao speech_to_text_citrinet export \\\n",
    "     -e $SPECS_DIR/speech_to_text_citrinet/export.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/citrinet/finetune/checkpoints/finetuned-model.tlt \\\n",
    "     -r $RESULTS_DIR/citrinet/export \\\n",
    "     export_format=ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ASR Inference using the checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ASR Inference with TAO Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to run inference on the tlt checkpoint with TAO Toolkit. \n",
    " For real-time inference and best latency, we need to deploy this model on Riva, which would be covered in the next tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets listen to the audio first\n",
    "path = os.environ[\"HOST_DATA_DIR\"] + '/an4_converted/wavs/an407-fcaw-b.wav'\n",
    "ipd.Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get the ground truth transcript for this sample\n",
    "import json\n",
    "\n",
    "def read_manifest(path):\n",
    "    manifest = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            data = json.loads(line)\n",
    "            manifest.append(data)\n",
    "    return manifest\n",
    "\n",
    "path = os.environ[\"HOST_DATA_DIR\"] + '/an4_converted/test_manifest.json'\n",
    "path_in_manifest = DATA_DIR + '/an4_converted/wavs/an407-fcaw-b.wav'\n",
    "\n",
    "manifest = read_manifest(path)\n",
    "transcript = [x['text'] for x in manifest if x[\"audio_filepath\"] == path_in_manifest]\n",
    "\n",
    "print(\"Ground truth transcript: \", transcript)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao speech_to_text_citrinet infer \\\n",
    "     -e $SPECS_DIR/speech_to_text_citrinet/infer.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/citrinet/finetune/checkpoints/finetuned-model.tlt \\\n",
    "     -r $RESULTS_DIR/citrinet/infer \\\n",
    "     file_paths=[$DATA_DIR/an4_converted/wavs/an407-fcaw-b.wav]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can upload your recorded `.wav` file and provide its path to the `file_paths` argument in the cell above to get the transcribed speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've fine-tuned Citrinet accoustic model, we can now deploy this custom model to NVIDIA Riva.\n",
    "\n",
    "Make sure to keep the path of `asr-model.riva` handy for deployment i.e. `asr_am_finetuning/results/citrinet/riva/asr-model.riva`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
